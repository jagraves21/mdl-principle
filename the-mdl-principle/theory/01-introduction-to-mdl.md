# An Introduction to the MDL Principle

The Minimum Description Length (MDL) principle is an approach to model selection and inference based on algorithmic complexity and information theory. The principle asserts that the best explanation of the data is one that admits the shortest description.

More formally, the MDL principle can be expressed as an optimization problem:

```math
M^{*} = \arg\min_{M} \mathcal{L}_{\text{MDL}}(M)
```

where the **MDL loss** is defined as:

```math
\mathcal{L}_{\text{MDL}}(M) = L(M) + L(D \mid M).
```

Here:

- $`M`$ is the candidate model,
- $`D`$ is the observed data,
- $`L(M)`$ is the description length of the model, and
- $`L(D \mid M)`$ is the description length of the data given the model.

The goal is to find the model $`M^{*}`$ that minimizes the total description length of the model and the data given the model. For now, we leave the concept of description length somewhat vague and will formalize it later.

## 1. Intuition and Model Complexity

Intuitively, the MDL objective attempts to achieve a balance between model complexity and the explanation of the data. A complex model that fits the data perfectly will likely have a very long description, potentially no shorter than the description of the original data itself. Conversely, a simple model will have a short description but will require a long description of the data given the model. Minimizing the MDL loss $`\mathcal{L}_{\text{MDL}}`$ therefore favors models that achieve an appropriate trade-off between model complexity and the explanatory power of the data.

For example, suppose we have a dataset $`D = \{ (x_i, y_i) \}_{1 \leq i \leq n}`$ that is a collection of $`x`$-$`y`$ points. We can attempt to model the data using polynomial regression:

```math
y = a_{0} + a_{1} x + a_{2} x^{2} + \dots + a_{d} x^{d},
```

where $`d`$ is the degree of the polynomial.

Once a model has been selected for the data, we can describe the data using that model. In polynomial regression, a natural way to do this is by describing the residuals: for each datapoint $`(x_{i}, y_{i})`$ in $`D`$, we compute the predicted value $`\hat{y}_{i}`$ using the model, and the residual is $`y_{i} - \hat{y}_{i}`$.

- A low-degree polynomial (underfitting) has a small model description $`L(M)`$, but the residuals require a larger description $`L(D \mid M)`$.
- A high-degree polynomial (overfitting) fits the data closely, resulting in a larger $`L(M)`$ due to many coefficients, but the residuals can be described with a smaller $`L(D \mid M)`$.

The MDL principle selects the polynomial degree $`d^{*}`$ that minimizes the total description length:

```math
\mathcal{L}_{\text{MDL}}(M) = L(M) + L(D \mid M),
```

balancing model complexity with the ability to explain the data accurately.

## 2. Two Interpretations of "Description"

Up until now, the notion of a description and its length has been intentionally left somewhat vague. In MDL theory, there are two traditional views of descriptions:

- **Algorithmic Descriptions:** Descriptions are formal languages or computer programs that can be used to generate the data.
- **Statistical Descriptions:** Descriptions are binary encodings of the data, typically based on probabilities and the lengths of codewords.

In both interpretations, models can be viewed as compression devices. Learning is therefore equivalent to compression: the better the model, the more patterns it captures in the data, and the more concisely the residual information can be described.

## 3. Algorithmic MDL

In algorithmic MDL, a description of data is defined as a formal procedure (i.e., a computer program) that, when executed on a universal computing device (e.g., a Turing machine), reproduces the dataset exactly. This program can include instructions, rules, or algorithms that generate each data point without relying on probabilistic assumptions. The length of such a program provides a measure of how complex or structured the data is. For example, the sequence $`(1, 2, 3, \dots, 100)`$ can be generated by a short program, whereas a random sequence of 100 numbers will require a much longer program.

### 3.1. Kolmogorov Complexity

Algorithmic MDL is formalized through _Kolmogorov complexity_. The Kolmogorov complexity $`K(D)`$ of a dataset $`D`$ is the length, in bits, of the shortest program $`p`$ that outputs $`D`$ on a fixed universal Turing machine $`U`$:

```math
K(D) = \min \{ |p| : U(p) = D \},
```

where $`|p|`$ denotes the program length. Kolmogorov complexity captures the _intrinsic information content_ of the data: highly regular data can be generated by short programs, while irregular or random data requires longer programs.

Some key aspects of algorithmic MDL include:

- The choice of universal Turing machine affects $`K(D)`$ only up to a fixed additive constant, ensuring _universality_.
- $`K(D)`$ represents a theoretical lower bound on any algorithmic description of the data.
- Although uncomputable in practice, it provides a rigorous foundation for the shortest possible description of any dataset.

Under algorithmic MDL, model selection can be interpreted as finding the shortest program that generates the data under different candidate models. Each model $`M`$ is represented as a program with length $`L(M)`$, and discrepancies between the model and actual data are encoded as "residuals" $`L(D \mid M)`$. The total description length is then

```math
\mathcal{L}_{\text{MDL}}(M) = L(M) + L(D \mid M),
```

and the MDL principle selects the model $`M^{*}`$ that minimizes this sum. A model that is too simple may have small $`L(M)`$ but large $`L(D \mid M)`$ (underfitting), while a model that is too complex may have large $`L(M)`$ but small $`L(D \mid M)`$ (overfitting). Algorithmic MDL thus formalizes the balance between simplicity and explanatory power: the best model admits the shortest combined program for model and data.

It is worth mentioning that exact Kolmogorov complexity is uncomputable. Nevertheless, it motivates practical approximations, such as computable codes or two-part descriptions, which aim to follow the algorithmic MDL principle.

### 3.2. Summary

Algorithmic MDL formalizes the intuition that learning is compression. Key points:

- **Compression is Learning:** The best description of data is the program with the smallest length that reproduces the data, capturing all regularities while leaving minimal residual information.
- **Model-Independent:** The quality of a description depends only on the intrinsic complexity of the data, without requiring probabilistic or model assumptions.
- **Theoretical Bound:** Exact Kolmogorov complexity is uncomputable but provides a fundamental lower limit on description length.
- **Simplicity and Explanation:** A good description balances brevity and explanatory power, unifying structure, simplicity, and insight across datasets.

## 4. Statistical MDL

Statistical MDL provides a practical, computable realization of the MDL principle by focusing on explicit coding schemes for data. Rather than viewing descriptions as abstract programs, descriptions are treated as encodings, and models are evaluated by how efficiently they allow the data to be encoded. The core principle remains unchanged: the best model is the one that minimizes the total length of the model description and the encoded data it explains.

Under this perspective, a model $`M`$ is not viewed as a data-generating mechanism. Instead, a model defines a coding scheme for data. Choosing a model means choosing a set of rules that determine how many bits are required to describe datasets that conform well to the model, and how many bits are required to describe deviations from it.

As before, model selection is formulated as minimizing the total description length

```math
\mathcal{L}_{\text{MDL}}(M) = L(M) + L(D \mid M),
```

where $`L(M)`$ is the number of bits required to encode the model itself, and $`L(D \mid M)`$ is the number of bits required to encode the data after accounting for the structure imposed by the model.

### 4.1. Coding the Model

The model itself must be encoded. This typically requires a code capable of describing both the model class and its parameter values. More complex models (for example, models with more parameters) generally require more bits to describe, reflecting their greater flexibility. This naturally penalizes overly flexible models, even if they fit the data closely, since their longer model descriptions may outweigh any savings achieved when encoding the data. The total description length therefore balances the cost of describing structure against the cost of describing unexplained variations from that structure.

For instance, consider polynomial regression. A polynomial can be completely determined by its coefficients $`a_{0}, a_{1}, \dots, a_{d}`$, so the model is specified by its degree $`d`$ and the coefficients. Encoding the model requires describing both the degree of the polynomial and the numerical values of its coefficients to some finite precision. Higher-degree polynomials require more coefficients, which increases $`L(M)`$ and reflects the additional flexibility of the model. Even if a high-degree polynomial fits the data very closely, the cost of encoding all its parameters contributes to the total description length, naturally penalizing overly complex models.

### 4.2. Coding the Data Given a Model

Once a model has been fixed, it induces a particular way of encoding data. Regularities captured by the model allow parts of the data to be described implicitly, while the remaining information must be encoded explicitly.

For example, in regression, the model produces predictions, which in turn give rise to the residuals (i.e., the differences between the observed and predicted values). To describe the data given the model, only the residuals need to be encoded using an appropriate coding scheme. A model that captures the dominant structure of the data yields small residuals that can be encoded efficiently, while a poor model produces large residuals that require longer descriptions. The effectiveness of the model is therefore measured by how efficiently the combination of model and residuals compresses the data.

### 4.3. Extensions and Practical Considerations

So far, we have described how to encode the model itself and how to encode the data given the model. Importantly, statistical MDL does not require specifying a concrete bit-level code for either the model or the data. Instead, it suffices to reason about _code lengths_. For any reasonable coding scheme, the lengths assigned to model parameters and residuals can be treated as well-defined quantities up to negligible overhead. This allows MDL to compare models based on their implied description lengths, without ever constructing explicit codes. This conceptual perspective also sets the stage for two common extensions: universal codes and the probabilistic notation frequently used in MDL literature.

#### 4.3.1. Universal and Distribution-Free Codes

In many situations, we do not know the distribution of the data or the residuals produced by a model. Statistical MDL accommodates this by using universal codes, such as universal integer codes or other distribution-free coding schemes. These codes adapt to the observed data and provide effective compression without requiring detailed prior knowledge. Universal codes enable model comparison even when explicit assumptions about the data are unavailable. From the MDL perspective, they are not fallback solutions, but principled tools for assigning code lengths under minimal assumptions.

#### 4.3.2. The Appearance of Probabilities

Although statistical MDL is fundamentally about codes, it is often expressed in the language of probabilities. This is not because MDL assumes the data is generated randomly, but because probabilities provide a convenient way to characterize optimal code lengths (here, _optimal_ simply means achieving the shortest expected code length for a given set of data or residuals). For example, Shannon's source coding theorem implies that, for any well-designed code, the length of a codeword can be expressed as the negative logarithm of its probability, $`-\log(\cdot)`$. Writing code lengths this way allows statistical MDL to leverage tools from information theory and statistics without changing its core interpretation: learning is compression, and probabilities serve only as a compact way to describe code lengths.

### 4.4. Summary

Statistical MDL formalizes the intuition that learning is compression using practical, computable codes. Key points:

- **Compression is Learning:** The goal is to encode both the model and the data (or residuals) as efficiently as possible. The more the data can be compressed, the better it has been understood.
- **Model vs. Data:** The model captures structure and determines residuals; the residuals are then encoded using a suitable coding scheme.
- **Practical Codes:** Statistical MDL does not require constructing literal bit-level codes; reasoning about code lengths is sufficient. Universal codes allow encoding with minimal assumptions, even when the data distribution is unknown.
- **Probabilities as Notation:** Probabilities often appear in statistical MDL formulas, but they are simply a convenient shorthand for expressing code lengths (e.g., $`-\log(\cdot)`$). They do not imply generative assumptions about the data.
- **Balancing Complexity and Fit:** Longer model descriptions penalize overly complex models, while shorter residual encodings reward models that capture the dominant structure of the data. The total description length achieves a principled balance between these factors.

Statistical MDL thus provides a practical framework for model selection that unifies simplicity, explanatory power, and compression in a single measure.

## 5. Relation to Other Concepts

The MDL principle shares close connections with several standard approaches in statistics and machine learning:

- **Maximum Likelihood Estimation (MLE):** Minimizing the description length of the data given a model often coincides with [maximizing the likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) of the data under that model. The code length of the residuals can be expressed in terms of the negative log-likelihood, so selecting the model that minimizes total description length aligns with choosing the maximum likelihood model with a [regularization penalty](https://en.wikipedia.org/wiki/Regularization_(mathematics)).

- **Bayesian Inference:** [Bayesian model selection](https://en.wikipedia.org/wiki/Bayesian_model_selection) incorporates priors over model parameters, balancing fit and complexity. Statistical MDL achieves a similar trade-off from a coding perspective: the description length of the model acts like an implicit prior penalizing more complex models.

- **Regularization and Information Criteria:** Popular model selection techniques such as [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) and [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) can be interpreted as approximations to MDL. For example, when fitting polynomial regressions, these criteria add penalties proportional to the number of parameters, reflecting the same principle that overly complex models should be penalized to avoid overfitting.

In this way, the MDL principle provides a unifying, compression-based framework that formalizes and generalizes many common statistical methods for model selection and regularization.

